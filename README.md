# ai-safety-benchmark
Evaluate LLMs for safe completions, refusals, and toxicity
